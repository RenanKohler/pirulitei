/* 
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * OpenAPI spec version: 2.0.0
 * 
 * Generated by: https://github.com/swagger-api/swagger-codegen.git
 */
using System;
using System.Linq;
using System.IO;
using System.Text;
using System.Text.RegularExpressions;
using System.Collections;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Runtime.Serialization;
using Newtonsoft.Json;
using Newtonsoft.Json.Converters;
using System.ComponentModel.DataAnnotations;
using SwaggerDateConverter = IO.Swagger.Client.SwaggerDateConverter;
namespace IO.Swagger.Model
{
    /// <summary>
    /// CreateChatCompletionRequest
    /// </summary>
    [DataContract]
        public partial class CreateChatCompletionRequest :  IEquatable<CreateChatCompletionRequest>, IValidatableObject
    {
        /// <summary>
        /// Initializes a new instance of the <see cref="CreateChatCompletionRequest" /> class.
        /// </summary>
        /// <param name="messages">A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models). (required).</param>
        /// <param name="model">ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API. (required).</param>
        /// <param name="frequencyPenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#x27;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)  (default to 0).</param>
        /// <param name="logitBias">Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. .</param>
        /// <param name="maxTokens">The maximum number of [tokens](/tokenizer) to generate in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#x27;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. .</param>
        /// <param name="n">How many chat completion choices to generate for each input message. (default to 1).</param>
        /// <param name="presencePenalty">Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#x27;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)  (default to 0).</param>
        /// <param name="responseFormat">responseFormat.</param>
        /// <param name="seed">This feature is in Beta.  If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. .</param>
        /// <param name="stop">Up to 4 sequences where the API will stop generating further tokens. .</param>
        /// <param name="stream">If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).  (default to false).</param>
        /// <param name="temperature">What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both.  (default to 1).</param>
        /// <param name="topP">An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both.  (default to 1).</param>
        /// <param name="tools">A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. .</param>
        /// <param name="toolChoice">toolChoice.</param>
        /// <param name="user">A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). .</param>
        /// <param name="functionCall">Deprecated in favor of &#x60;tool_choice&#x60;.  Controls which (if any) function is called by the model. &#x60;none&#x60; means the model will not call a function and instead generates a message. &#x60;auto&#x60; means the model can pick between generating a message or calling a function. Specifying a particular function via &#x60;{\&quot;name\&quot;: \&quot;my_function\&quot;}&#x60; forces the model to call that function.  &#x60;none&#x60; is the default when no functions are present. &#x60;auto&#x60;&#x60; is the default if functions are present. .</param>
        /// <param name="functions">Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. .</param>
        public CreateChatCompletionRequest(List<ChatCompletionRequestMessage> messages = default(List<ChatCompletionRequestMessage>), AnyOfCreateChatCompletionRequestModel model = default(AnyOfCreateChatCompletionRequestModel), decimal? frequencyPenalty = 0, Dictionary<string, int?> logitBias = default(Dictionary<string, int?>), int? maxTokens = default(int?), int? n = 1, decimal? presencePenalty = 0, CreateChatCompletionRequestResponseFormat responseFormat = default(CreateChatCompletionRequestResponseFormat), int? seed = default(int?), OneOfCreateChatCompletionRequestStop stop = default(OneOfCreateChatCompletionRequestStop), bool? stream = false, decimal? temperature = 1, decimal? topP = 1, List<ChatCompletionTool> tools = default(List<ChatCompletionTool>), ChatCompletionToolChoiceOption toolChoice = default(ChatCompletionToolChoiceOption), string user = default(string), OneOfCreateChatCompletionRequestFunctionCall functionCall = default(OneOfCreateChatCompletionRequestFunctionCall), List<ChatCompletionFunctions> functions = default(List<ChatCompletionFunctions>))
        {
            // to ensure "messages" is required (not null)
            if (messages == null)
            {
                throw new InvalidDataException("messages is a required property for CreateChatCompletionRequest and cannot be null");
            }
            else
            {
                this.Messages = messages;
            }
            // to ensure "model" is required (not null)
            if (model == null)
            {
                throw new InvalidDataException("model is a required property for CreateChatCompletionRequest and cannot be null");
            }
            else
            {
                this.Model = model;
            }
            // use default value if no "frequencyPenalty" provided
            if (frequencyPenalty == null)
            {
                this.FrequencyPenalty = 0;
            }
            else
            {
                this.FrequencyPenalty = frequencyPenalty;
            }
            this.LogitBias = logitBias;
            this.MaxTokens = maxTokens;
            // use default value if no "n" provided
            if (n == null)
            {
                this.N = 1;
            }
            else
            {
                this.N = n;
            }
            // use default value if no "presencePenalty" provided
            if (presencePenalty == null)
            {
                this.PresencePenalty = 0;
            }
            else
            {
                this.PresencePenalty = presencePenalty;
            }
            this.ResponseFormat = responseFormat;
            this.Seed = seed;
            this.Stop = stop;
            // use default value if no "stream" provided
            if (stream == null)
            {
                this.Stream = false;
            }
            else
            {
                this.Stream = stream;
            }
            // use default value if no "temperature" provided
            if (temperature == null)
            {
                this.Temperature = 1;
            }
            else
            {
                this.Temperature = temperature;
            }
            // use default value if no "topP" provided
            if (topP == null)
            {
                this.TopP = 1;
            }
            else
            {
                this.TopP = topP;
            }
            this.Tools = tools;
            this.ToolChoice = toolChoice;
            this.User = user;
            this.FunctionCall = functionCall;
            this.Functions = functions;
        }
        
        /// <summary>
        /// A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
        /// </summary>
        /// <value>A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).</value>
        [DataMember(Name="messages", EmitDefaultValue=false)]
        public List<ChatCompletionRequestMessage> Messages { get; set; }

        /// <summary>
        /// ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
        /// </summary>
        /// <value>ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.</value>
        [DataMember(Name="model", EmitDefaultValue=false)]
        public AnyOfCreateChatCompletionRequestModel Model { get; set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#x27;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#x27;s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details) </value>
        [DataMember(Name="frequency_penalty", EmitDefaultValue=false)]
        public decimal? FrequencyPenalty { get; set; }

        /// <summary>
        /// Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. 
        /// </summary>
        /// <value>Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. </value>
        [DataMember(Name="logit_bias", EmitDefaultValue=false)]
        public Dictionary<string, int?> LogitBias { get; set; }

        /// <summary>
        /// The maximum number of [tokens](/tokenizer) to generate in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#x27;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. 
        /// </summary>
        /// <value>The maximum number of [tokens](/tokenizer) to generate in the chat completion.  The total length of input tokens and generated tokens is limited by the model&#x27;s context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens. </value>
        [DataMember(Name="max_tokens", EmitDefaultValue=false)]
        public int? MaxTokens { get; set; }

        /// <summary>
        /// How many chat completion choices to generate for each input message.
        /// </summary>
        /// <value>How many chat completion choices to generate for each input message.</value>
        [DataMember(Name="n", EmitDefaultValue=false)]
        public int? N { get; set; }

        /// <summary>
        /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#x27;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details) 
        /// </summary>
        /// <value>Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#x27;s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details) </value>
        [DataMember(Name="presence_penalty", EmitDefaultValue=false)]
        public decimal? PresencePenalty { get; set; }

        /// <summary>
        /// Gets or Sets ResponseFormat
        /// </summary>
        [DataMember(Name="response_format", EmitDefaultValue=false)]
        public CreateChatCompletionRequestResponseFormat ResponseFormat { get; set; }

        /// <summary>
        /// This feature is in Beta.  If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. 
        /// </summary>
        /// <value>This feature is in Beta.  If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same &#x60;seed&#x60; and parameters should return the same result. Determinism is not guaranteed, and you should refer to the &#x60;system_fingerprint&#x60; response parameter to monitor changes in the backend. </value>
        [DataMember(Name="seed", EmitDefaultValue=false)]
        public int? Seed { get; set; }

        /// <summary>
        /// Up to 4 sequences where the API will stop generating further tokens. 
        /// </summary>
        /// <value>Up to 4 sequences where the API will stop generating further tokens. </value>
        [DataMember(Name="stop", EmitDefaultValue=false)]
        public OneOfCreateChatCompletionRequestStop Stop { get; set; }

        /// <summary>
        /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). 
        /// </summary>
        /// <value>If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a &#x60;data: [DONE]&#x60; message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions). </value>
        [DataMember(Name="stream", EmitDefaultValue=false)]
        public bool? Stream { get; set; }

        /// <summary>
        /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. 
        /// </summary>
        /// <value>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or &#x60;top_p&#x60; but not both. </value>
        [DataMember(Name="temperature", EmitDefaultValue=false)]
        public decimal? Temperature { get; set; }

        /// <summary>
        /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. 
        /// </summary>
        /// <value>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or &#x60;temperature&#x60; but not both. </value>
        [DataMember(Name="top_p", EmitDefaultValue=false)]
        public decimal? TopP { get; set; }

        /// <summary>
        /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. 
        /// </summary>
        /// <value>A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. </value>
        [DataMember(Name="tools", EmitDefaultValue=false)]
        public List<ChatCompletionTool> Tools { get; set; }

        /// <summary>
        /// Gets or Sets ToolChoice
        /// </summary>
        [DataMember(Name="tool_choice", EmitDefaultValue=false)]
        public ChatCompletionToolChoiceOption ToolChoice { get; set; }

        /// <summary>
        /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). 
        /// </summary>
        /// <value>A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). </value>
        [DataMember(Name="user", EmitDefaultValue=false)]
        public string User { get; set; }

        /// <summary>
        /// Deprecated in favor of &#x60;tool_choice&#x60;.  Controls which (if any) function is called by the model. &#x60;none&#x60; means the model will not call a function and instead generates a message. &#x60;auto&#x60; means the model can pick between generating a message or calling a function. Specifying a particular function via &#x60;{\&quot;name\&quot;: \&quot;my_function\&quot;}&#x60; forces the model to call that function.  &#x60;none&#x60; is the default when no functions are present. &#x60;auto&#x60;&#x60; is the default if functions are present. 
        /// </summary>
        /// <value>Deprecated in favor of &#x60;tool_choice&#x60;.  Controls which (if any) function is called by the model. &#x60;none&#x60; means the model will not call a function and instead generates a message. &#x60;auto&#x60; means the model can pick between generating a message or calling a function. Specifying a particular function via &#x60;{\&quot;name\&quot;: \&quot;my_function\&quot;}&#x60; forces the model to call that function.  &#x60;none&#x60; is the default when no functions are present. &#x60;auto&#x60;&#x60; is the default if functions are present. </value>
        [DataMember(Name="function_call", EmitDefaultValue=false)]
        public OneOfCreateChatCompletionRequestFunctionCall FunctionCall { get; set; }

        /// <summary>
        /// Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. 
        /// </summary>
        /// <value>Deprecated in favor of &#x60;tools&#x60;.  A list of functions the model may generate JSON inputs for. </value>
        [DataMember(Name="functions", EmitDefaultValue=false)]
        public List<ChatCompletionFunctions> Functions { get; set; }

        /// <summary>
        /// Returns the string presentation of the object
        /// </summary>
        /// <returns>String presentation of the object</returns>
        public override string ToString()
        {
            var sb = new StringBuilder();
            sb.Append("class CreateChatCompletionRequest {\n");
            sb.Append("  Messages: ").Append(Messages).Append("\n");
            sb.Append("  Model: ").Append(Model).Append("\n");
            sb.Append("  FrequencyPenalty: ").Append(FrequencyPenalty).Append("\n");
            sb.Append("  LogitBias: ").Append(LogitBias).Append("\n");
            sb.Append("  MaxTokens: ").Append(MaxTokens).Append("\n");
            sb.Append("  N: ").Append(N).Append("\n");
            sb.Append("  PresencePenalty: ").Append(PresencePenalty).Append("\n");
            sb.Append("  ResponseFormat: ").Append(ResponseFormat).Append("\n");
            sb.Append("  Seed: ").Append(Seed).Append("\n");
            sb.Append("  Stop: ").Append(Stop).Append("\n");
            sb.Append("  Stream: ").Append(Stream).Append("\n");
            sb.Append("  Temperature: ").Append(Temperature).Append("\n");
            sb.Append("  TopP: ").Append(TopP).Append("\n");
            sb.Append("  Tools: ").Append(Tools).Append("\n");
            sb.Append("  ToolChoice: ").Append(ToolChoice).Append("\n");
            sb.Append("  User: ").Append(User).Append("\n");
            sb.Append("  FunctionCall: ").Append(FunctionCall).Append("\n");
            sb.Append("  Functions: ").Append(Functions).Append("\n");
            sb.Append("}\n");
            return sb.ToString();
        }
  
        /// <summary>
        /// Returns the JSON string presentation of the object
        /// </summary>
        /// <returns>JSON string presentation of the object</returns>
        public virtual string ToJson()
        {
            return JsonConvert.SerializeObject(this, Formatting.Indented);
        }

        /// <summary>
        /// Returns true if objects are equal
        /// </summary>
        /// <param name="input">Object to be compared</param>
        /// <returns>Boolean</returns>
        public override bool Equals(object input)
        {
            return this.Equals(input as CreateChatCompletionRequest);
        }

        /// <summary>
        /// Returns true if CreateChatCompletionRequest instances are equal
        /// </summary>
        /// <param name="input">Instance of CreateChatCompletionRequest to be compared</param>
        /// <returns>Boolean</returns>
        public bool Equals(CreateChatCompletionRequest input)
        {
            if (input == null)
                return false;

            return 
                (
                    this.Messages == input.Messages ||
                    this.Messages != null &&
                    input.Messages != null &&
                    this.Messages.SequenceEqual(input.Messages)
                ) && 
                (
                    this.Model == input.Model ||
                    (this.Model != null &&
                    this.Model.Equals(input.Model))
                ) && 
                (
                    this.FrequencyPenalty == input.FrequencyPenalty ||
                    (this.FrequencyPenalty != null &&
                    this.FrequencyPenalty.Equals(input.FrequencyPenalty))
                ) && 
                (
                    this.LogitBias == input.LogitBias ||
                    this.LogitBias != null &&
                    input.LogitBias != null &&
                    this.LogitBias.SequenceEqual(input.LogitBias)
                ) && 
                (
                    this.MaxTokens == input.MaxTokens ||
                    (this.MaxTokens != null &&
                    this.MaxTokens.Equals(input.MaxTokens))
                ) && 
                (
                    this.N == input.N ||
                    (this.N != null &&
                    this.N.Equals(input.N))
                ) && 
                (
                    this.PresencePenalty == input.PresencePenalty ||
                    (this.PresencePenalty != null &&
                    this.PresencePenalty.Equals(input.PresencePenalty))
                ) && 
                (
                    this.ResponseFormat == input.ResponseFormat ||
                    (this.ResponseFormat != null &&
                    this.ResponseFormat.Equals(input.ResponseFormat))
                ) && 
                (
                    this.Seed == input.Seed ||
                    (this.Seed != null &&
                    this.Seed.Equals(input.Seed))
                ) && 
                (
                    this.Stop == input.Stop ||
                    (this.Stop != null &&
                    this.Stop.Equals(input.Stop))
                ) && 
                (
                    this.Stream == input.Stream ||
                    (this.Stream != null &&
                    this.Stream.Equals(input.Stream))
                ) && 
                (
                    this.Temperature == input.Temperature ||
                    (this.Temperature != null &&
                    this.Temperature.Equals(input.Temperature))
                ) && 
                (
                    this.TopP == input.TopP ||
                    (this.TopP != null &&
                    this.TopP.Equals(input.TopP))
                ) && 
                (
                    this.Tools == input.Tools ||
                    this.Tools != null &&
                    input.Tools != null &&
                    this.Tools.SequenceEqual(input.Tools)
                ) && 
                (
                    this.ToolChoice == input.ToolChoice ||
                    (this.ToolChoice != null &&
                    this.ToolChoice.Equals(input.ToolChoice))
                ) && 
                (
                    this.User == input.User ||
                    (this.User != null &&
                    this.User.Equals(input.User))
                ) && 
                (
                    this.FunctionCall == input.FunctionCall ||
                    (this.FunctionCall != null &&
                    this.FunctionCall.Equals(input.FunctionCall))
                ) && 
                (
                    this.Functions == input.Functions ||
                    this.Functions != null &&
                    input.Functions != null &&
                    this.Functions.SequenceEqual(input.Functions)
                );
        }

        /// <summary>
        /// Gets the hash code
        /// </summary>
        /// <returns>Hash code</returns>
        public override int GetHashCode()
        {
            unchecked // Overflow is fine, just wrap
            {
                int hashCode = 41;
                if (this.Messages != null)
                    hashCode = hashCode * 59 + this.Messages.GetHashCode();
                if (this.Model != null)
                    hashCode = hashCode * 59 + this.Model.GetHashCode();
                if (this.FrequencyPenalty != null)
                    hashCode = hashCode * 59 + this.FrequencyPenalty.GetHashCode();
                if (this.LogitBias != null)
                    hashCode = hashCode * 59 + this.LogitBias.GetHashCode();
                if (this.MaxTokens != null)
                    hashCode = hashCode * 59 + this.MaxTokens.GetHashCode();
                if (this.N != null)
                    hashCode = hashCode * 59 + this.N.GetHashCode();
                if (this.PresencePenalty != null)
                    hashCode = hashCode * 59 + this.PresencePenalty.GetHashCode();
                if (this.ResponseFormat != null)
                    hashCode = hashCode * 59 + this.ResponseFormat.GetHashCode();
                if (this.Seed != null)
                    hashCode = hashCode * 59 + this.Seed.GetHashCode();
                if (this.Stop != null)
                    hashCode = hashCode * 59 + this.Stop.GetHashCode();
                if (this.Stream != null)
                    hashCode = hashCode * 59 + this.Stream.GetHashCode();
                if (this.Temperature != null)
                    hashCode = hashCode * 59 + this.Temperature.GetHashCode();
                if (this.TopP != null)
                    hashCode = hashCode * 59 + this.TopP.GetHashCode();
                if (this.Tools != null)
                    hashCode = hashCode * 59 + this.Tools.GetHashCode();
                if (this.ToolChoice != null)
                    hashCode = hashCode * 59 + this.ToolChoice.GetHashCode();
                if (this.User != null)
                    hashCode = hashCode * 59 + this.User.GetHashCode();
                if (this.FunctionCall != null)
                    hashCode = hashCode * 59 + this.FunctionCall.GetHashCode();
                if (this.Functions != null)
                    hashCode = hashCode * 59 + this.Functions.GetHashCode();
                return hashCode;
            }
        }

        /// <summary>
        /// To validate all properties of the instance
        /// </summary>
        /// <param name="validationContext">Validation context</param>
        /// <returns>Validation Result</returns>
        IEnumerable<System.ComponentModel.DataAnnotations.ValidationResult> IValidatableObject.Validate(ValidationContext validationContext)
        {
            yield break;
        }
    }
}
